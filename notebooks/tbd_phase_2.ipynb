{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "711646c422baab35",
   "metadata": {},
   "source": [
    "# TBD Phase 2: Performance & Computing Models\n",
    "\n",
    "## Introduction\n",
    "In this lab, you will compare the performance and computing models of four popular data processing engines: **Polars, Pandas, DuckDB, and PySpark**.\n",
    "\n",
    "You will explore:\n",
    "- **Performance**: Single-node processing speed, parallel execution, and memory usage.\n",
    "- **Scalability**: How performance changes with the number of cores (single-node) and executors (cluster).\n",
    "- **Computing Models**: Out-of-core vs. In-memory processing, and Eager vs. Lazy execution.\n",
    "\n",
    "### Engine Capabilities\n",
    "The following table summarizes the key capabilities of the engines we will be testing. Use this as a reference.\n",
    "\n",
    "| Engine | Query Optimizer | Distributed | Arrow-backed | Out-of-Core | Parallel | APIs | GPU Support |\n",
    "|---|---|---|---|---|--|---|---|\n",
    "| **Pandas** | ❌ | ❌ | optional ≥ 2.0 | ❌ | ❌ | DataFrame | ❌ |\n",
    "| **Polars** | ✅ | ❌ | ✅ | ✅ | ✅ | DataFrame | ✅ (opt) |\n",
    "| **PySpark** | ✅ | ✅ | Pandas UDF/IO | ✅ | ✅ | SQL, DataFrame | ❌ (no GPU) |\n",
    "| **DuckDB** | ✅ | ❌ | ✅ | ✅ | ❌ | SQL, Relational API | ❌ |\n",
    "\n",
    "## Prerequisites\n",
    "Ensure you have the necessary libraries installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f03c89d6004dfa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/pkutyl/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/pip/__main__.py\", line 22, in <module>\n",
      "    from pip._internal.cli.main import main as _main\n",
      "  File \"/Users/pkutyl/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/pip/_internal/cli/main.py\", line 11, in <module>\n",
      "    from pip._internal.cli.autocompletion import autocomplete\n",
      "  File \"/Users/pkutyl/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/pip/_internal/cli/autocompletion.py\", line 12, in <module>\n",
      "    from pip._internal.cli.main_parser import create_main_parser\n",
      "  File \"/Users/pkutyl/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/pip/_internal/cli/main_parser.py\", line 12, in <module>\n",
      "    from pip._internal.commands import commands_dict, get_similar_commands\n",
      "  File \"/Users/pkutyl/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/pip/_internal/commands/__init__.py\", line 11, in <module>\n",
      "    from pip._internal.cli.base_command import Command\n",
      "  File \"/Users/pkutyl/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/pip/_internal/cli/base_command.py\", line 6, in <module>\n",
      "    import logging.config\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/config.py\", line 337, in <module>\n",
      "    class ConvertingDict(dict, ConvertingMixin):\n",
      "KeyboardInterrupt\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install polars pandas duckdb pyspark faker deltalake memory_profiler pyarrow matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d4c5b1b58dc798a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/20 21:23:58 WARN Utils: Your hostname, MacBook-Pro-Pawel.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.171 instead (on interface en0)\n",
      "26/01/20 21:23:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/20 21:23:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from pyspark.sql import SparkSession\n",
    "from faker import Faker\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "from memory_profiler import memory_usage\n",
    "# Initialize Spark (Single Node)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigDataLab2\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.execution.memory\", \"8g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fdbc3b29449e48",
   "metadata": {},
   "source": [
    "## Part 1: Data Generation\n",
    "\n",
    "We will generate a synthetic dataset simulating social media posts with a rich schema.\n",
    "\n",
    "**Schema**:\n",
    "- `post_id` (String): Unique identifier.\n",
    "- `user_id` (Integer): User identifier.\n",
    "- `timestamp` (DateTime): Time of post.\n",
    "- `content` (String): Text content.\n",
    "- `likes` (Integer): Number of likes.\n",
    "- `views` (Integer): Number of views.\n",
    "- `category` (String): Post category.\n",
    "- `tags` (List[String]): Hashtags.\n",
    "- `location` (String): User location.\n",
    "- `device` (String): Device used (Mobile, Web, etc.).\n",
    "- `latency` (Float): Network latency.\n",
    "- `error_rate` (Float): Error rate during upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f6706879ef7347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 5000000 records...\n",
      "Writing to Parquet...\n",
      "Data saved to social_media_data.parquet\n"
     ]
    }
   ],
   "source": [
    "def generate_data(num_records=1_000_000, output_path=\"social_media_data.parquet\"):\n",
    "    fake = Faker()\n",
    "    \n",
    "    print(f\"Generating {num_records} records...\")\n",
    "    \n",
    "    # Generate data using numpy for speed where possible\n",
    "    data = {\n",
    "        \"post_id\": [fake.uuid4() for _ in range(num_records)],\n",
    "        \"user_id\": np.random.randint(1, 100_000, num_records),\n",
    "        \"timestamp\": pd.date_range(start=\"2023-01-01\", periods=num_records, freq=\"s\").to_numpy().astype(\"datetime64[us]\"),\n",
    "        \"likes\": np.random.randint(0, 10_000, num_records),\n",
    "        \"views\": np.random.randint(0, 1_000_000, num_records),\n",
    "        \"category\": np.random.choice([\"Tech\", \"Health\", \"Travel\", \"Food\", \"Fashion\", \"Politics\", \"Sports\"], num_records),\n",
    "        \"tags\": [np.random.choice([\"#viral\", \"#new\", \"#trending\", \"#hot\", \"#update\"], size=np.random.randint(1, 4)).tolist() for _ in range(num_records)],\n",
    "        \"location\": np.random.choice([\"USA\", \"UK\", \"DE\", \"PL\", \"FR\", \"JP\", \"BR\"], num_records),\n",
    "        \"device\": np.random.choice([\"Mobile\", \"Desktop\", \"Tablet\"], num_records),\n",
    "        \"latency\": np.random.uniform(10.0, 500.0, num_records),\n",
    "        \"error_rate\": np.random.beta(1, 10, num_records),\n",
    "        \"content\": [fake.sentence() for _ in range(min(num_records, 1000))] * (num_records // 1000 + 1)\n",
    "    }\n",
    "    \n",
    "    # Trim to exact size\n",
    "    data[\"content\"] = data[\"content\"][:num_records]\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    print(\"Writing to Parquet...\")\n",
    "    df.to_parquet(output_path, engine=\"pyarrow\")\n",
    "    print(f\"Data saved to {output_path}\")\n",
    "\n",
    "# Generate 5 million records\n",
    "generate_data(num_records=5_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc9302afb73233",
   "metadata": {},
   "source": [
    "## Part 2: Measuring Performance\n",
    "\n",
    "### 2.1 Execution Time\n",
    "Use `%time` or `%timeit` to measure execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1853bedbac6744f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Performance Benchmark Example ---\n",
      "Pandas Load Time:\n",
      "CPU times: user 2.61 s, sys: 1.27 s, total: 3.88 s\n",
      "Wall time: 3.9 s\n",
      "\n",
      "Polars Load Time:\n",
      "CPU times: user 470 ms, sys: 209 ms, total: 678 ms\n",
      "Wall time: 459 ms\n",
      "\n",
      "DuckDB Query Time:\n",
      "┌──────────────┐\n",
      "│ count_star() │\n",
      "│    int64     │\n",
      "├──────────────┤\n",
      "│      5000000 │\n",
      "└──────────────┘\n",
      "\n",
      "CPU times: user 1.24 ms, sys: 3.67 ms, total: 4.91 ms\n",
      "Wall time: 21.2 ms\n",
      "\n",
      "Spark Load Time:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.58 ms, sys: 8.32 ms, total: 13.9 ms\n",
      "Wall time: 3.44 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Measuring time for all engines\n",
    "print(\"--- Performance Benchmark Example ---\")\n",
    "\n",
    "# Pandas\n",
    "print(\"Pandas Load Time:\")\n",
    "%time df_pd = pd.read_parquet(\"social_media_data.parquet\")\n",
    "\n",
    "# Polars\n",
    "print(\"\\nPolars Load Time:\")\n",
    "%time df_pl = pl.read_parquet(\"social_media_data.parquet\")\n",
    "\n",
    "# DuckDB\n",
    "print(\"\\nDuckDB Query Time:\")\n",
    "%time duckdb.sql(\"SELECT count(*) FROM 'social_media_data.parquet'\").show()\n",
    "\n",
    "# PySpark\n",
    "print(\"\\nSpark Load Time:\")\n",
    "%time df_spark = spark.read.parquet(\"social_media_data.parquet\"); df_spark.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e72523f0151fa3d",
   "metadata": {},
   "source": [
    "## Part 3: Student Tasks\n",
    "\n",
    "### Task 1: Performance & Scalability (Single Node)\n",
    "\n",
    "**Goal**: Benchmark the engines and test how they scale with available CPU cores.\n",
    "\n",
    "**Instructions**:\n",
    "1.  **Define Queries**: Create 3 distinct queries of your own choice. They should cover:\n",
    "    -   **Query A**: A simple aggregation (e.g., grouping by a categorical column and calculating means).\n",
    "    -   **Query B**: A window function or more complex transformation.\n",
    "    -   **Query C**: A join (e.g., self-join or join with a smaller generated table) with filtering.\n",
    "2.  **Benchmark**: Implement these queries in **Pandas, Polars, DuckDB, and PySpark**.\n",
    "    -   Measure **Execution Time** using `%time` or `time.time()`.\n",
    "    -   Measure **Peak Memory** usage using `memory_profiler` (e.g., `memory_usage()`).\n",
    "3.  **Scalability Test**: \n",
    "    -   Select **all engines** that support parallel execution on a single node (e.g., Polars, DuckDB).\n",
    "    -   Run **all 3 queries** with different numbers of threads/cores (e.g., 1, 2, 4, 8).\n",
    "    -   Plot the speedup for each query and engine.\n",
    "\n",
    "**Tip**: \n",
    "-   Polars: [polars.thread_pool_size](https://docs.pola.rs/api/python/stable/reference/api/polars.thread_pool_size.html) Please also note that *Thread configuration in Polars requires process restart*\n",
    "-   DuckDB: `PRAGMA threads=n`\n",
    "-   Spark: `master=\"local[n]\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c42c1fd440c982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas | QueryA | threads=1 | time=3.464s | peak_mb=548.88\n",
      "pandas | QueryB | threads=1 | time=16.634s | peak_mb=1992.95\n",
      "pandas | QueryC | threads=1 | time=2.409s | peak_mb=2493.38\n",
      "polars | QueryA | threads=1 | time=0.383s | peak_mb=2120.95\n",
      "polars | QueryB | threads=1 | time=2.887s | peak_mb=2047.41\n",
      "polars | QueryC | threads=1 | time=2.546s | peak_mb=2203.84\n",
      "duckdb | QueryA | threads=1 | time=0.580s | peak_mb=1352.70\n",
      "duckdb | QueryB | threads=1 | time=2.972s | peak_mb=1807.41\n",
      "duckdb | QueryC | threads=1 | time=0.510s | peak_mb=1332.19\n",
      "duckdb | QueryA | threads=2 | time=0.330s | peak_mb=1239.11\n",
      "duckdb | QueryB | threads=2 | time=1.816s | peak_mb=1150.59\n",
      "duckdb | QueryC | threads=2 | time=0.500s | peak_mb=893.50\n",
      "duckdb | QueryA | threads=4 | time=0.328s | peak_mb=812.92\n",
      "duckdb | QueryB | threads=4 | time=1.111s | peak_mb=858.91\n",
      "duckdb | QueryC | threads=4 | time=0.348s | peak_mb=855.86\n",
      "duckdb | QueryA | threads=8 | time=0.309s | peak_mb=855.84\n",
      "duckdb | QueryB | threads=8 | time=0.835s | peak_mb=937.47\n",
      "duckdb | QueryC | threads=8 | time=0.304s | peak_mb=933.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===================>                                       (1 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark | QueryA | threads=1 | time=2.624s | peak_mb=1029.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark | QueryB | threads=1 | time=21.859s | peak_mb=1055.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/20 21:27:21 WARN TaskSetManager: Lost task 0.0 in stage 8.0 (TID 9) (192.168.0.171 executor driver): TaskKilled (Stage cancelled: [SPARK_JOB_CANCELLED] Job 6 cancelled The corresponding SQL query has failed. SQLSTATE: XXKDA)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/pkutyl/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/py4j/clientserver.py\", line 535, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py\", line 719, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ~~~~~~~~~~~~~~~~~~~~^^^\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/pkutyl/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/pkutyl/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/py4j/clientserver.py\", line 566, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "        \"Error while sending or receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 164\u001b[39m\n\u001b[32m    163\u001b[39m fn = registry[engine]\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m elapsed, peak_mem = \u001b[43m_bench\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m results.append({\n\u001b[32m    166\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m: engine,\n\u001b[32m    167\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mthreads\u001b[39m\u001b[33m\"\u001b[39m: threads \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    170\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpeak_mb\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(peak_mem, \u001b[32m2\u001b[39m),\n\u001b[32m    171\u001b[39m })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 137\u001b[39m, in \u001b[36m_bench\u001b[39m\u001b[34m(run_fn)\u001b[39m\n\u001b[32m    136\u001b[39m t0 = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m mem_trace, _ = \u001b[43mmemory_usage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretval\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_children\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iterations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m elapsed = time.time() - t0\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/memory_profiler.py:379\u001b[39m, in \u001b[36mmemory_usage\u001b[39m\u001b[34m(proc, interval, timeout, timestamps, include_children, multiprocess, max_usage, retval, stream, backend, max_iterations)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m     returned = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    380\u001b[39m     parent_conn.send(\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# finish timing\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 124\u001b[39m, in \u001b[36mquery_c_spark\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery_c_spark\u001b[39m():\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    120\u001b[39m         \u001b[43mdf_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_dim_spark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcategory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mviews\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m&\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlikes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcategory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mavg\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlikes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/pyspark/sql/classic/dataframe.py:443\u001b[39m, in \u001b[36mDataFrame.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m._sc):\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m     sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:263\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31m<class 'str'>\u001b[39m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:2212\u001b[39m, in \u001b[36mInteractiveShell.showtraceback\u001b[39m\u001b[34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[39m\n\u001b[32m   2209\u001b[39m         traceback.print_exc()\n\u001b[32m   2210\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2212\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.call_pdb:\n\u001b[32m   2214\u001b[39m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[32m   2215\u001b[39m     \u001b[38;5;28mself\u001b[39m.debugger(force=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/ipykernel/zmqshell.py:673\u001b[39m, in \u001b[36mZMQInteractiveShell._showtraceback\u001b[39m\u001b[34m(self, etype, evalue, stb)\u001b[39m\n\u001b[32m    667\u001b[39m sys.stdout.flush()\n\u001b[32m    668\u001b[39m sys.stderr.flush()\n\u001b[32m    670\u001b[39m exc_content = {\n\u001b[32m    671\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtraceback\u001b[39m\u001b[33m\"\u001b[39m: stb,\n\u001b[32m    672\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mename\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype.\u001b[34m__name__\u001b[39m),\n\u001b[32m--> \u001b[39m\u001b[32m673\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mevalue\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    674\u001b[39m }\n\u001b[32m    676\u001b[39m dh = \u001b[38;5;28mself\u001b[39m.displayhook\n\u001b[32m    677\u001b[39m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[32m    678\u001b[39m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/py4j/protocol.py:472\u001b[39m, in \u001b[36mPy4JJavaError.__str__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    471\u001b[39m     gateway_client = \u001b[38;5;28mself\u001b[39m.java_exception._gateway_client\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m     answer = \u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    473\u001b[39m     return_value = get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    474\u001b[39m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[32m    475\u001b[39m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[32m    476\u001b[39m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/py4j/clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/py4j/clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/py4j/clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 61] Connection refused"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/IPython/core/async_helpers.py:128\u001b[39m, in \u001b[36m_pseudo_sync_runner\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[33;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[32m    122\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m \u001b[33;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3419\u001b[39m, in \u001b[36mInteractiveShell.run_cell_async\u001b[39m\u001b[34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[39m\n\u001b[32m   3414\u001b[39m     \u001b[38;5;28mself\u001b[39m.history_manager.store_output(execution_count)\n\u001b[32m   3415\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result.error_in_exec:\n\u001b[32m   3416\u001b[39m         \u001b[38;5;66;03m# Store formatted traceback and error details\u001b[39;00m\n\u001b[32m   3417\u001b[39m         \u001b[38;5;28mself\u001b[39m.history_manager.exceptions[\n\u001b[32m   3418\u001b[39m             execution_count\n\u001b[32m-> \u001b[39m\u001b[32m3419\u001b[39m         ] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_exception_for_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror_in_exec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3476\u001b[39m, in \u001b[36mInteractiveShell._format_exception_for_storage\u001b[39m\u001b[34m(self, exception, filename, running_compiled_code)\u001b[39m\n\u001b[32m   3472\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   3473\u001b[39m             \u001b[38;5;66;03m# In case formatting fails, fallback to Python's built-in formatting.\u001b[39;00m\n\u001b[32m   3474\u001b[39m             stb = traceback.format_exception(etype, evalue, tb)\n\u001b[32m-> \u001b[39m\u001b[32m3476\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mename\u001b[39m\u001b[33m\"\u001b[39m: etype.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mevalue\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mtraceback\u001b[39m\u001b[33m\"\u001b[39m: stb}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/py4j/protocol.py:472\u001b[39m, in \u001b[36mPy4JJavaError.__str__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    471\u001b[39m     gateway_client = \u001b[38;5;28mself\u001b[39m.java_exception._gateway_client\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m     answer = \u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    473\u001b[39m     return_value = get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    474\u001b[39m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[32m    475\u001b[39m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[32m    476\u001b[39m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/py4j/clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/py4j/clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tbd-grupowe/tbd-workshop-1/.venv/lib/python3.13/site-packages/py4j/clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "# Task 1: Performance & Scalability (Single Node)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F, Window\n",
    "\n",
    "DATA_PATH = \"social_media_data.parquet\"\n",
    "THREADS_TO_TEST = [1, 2, 4]\n",
    "\n",
    "_dim_pd = pd.DataFrame({\n",
    "    \"category\": [\"Tech\", \"Health\", \"Travel\", \"Food\", \"Fashion\", \"Politics\", \"Sports\"],\n",
    "    \"weight\": [1.2, 0.9, 1.1, 1.0, 1.05, 0.95, 1.15]\n",
    "})\n",
    "_dim_pl = pl.from_pandas(_dim_pd)\n",
    "_dim_spark = spark.createDataFrame(_dim_pd)\n",
    "\n",
    "df_pd = pd.read_parquet(DATA_PATH)\n",
    "df_pl = pl.read_parquet(DATA_PATH)\n",
    "df_spark = spark.read.parquet(DATA_PATH)\n",
    "\n",
    "_duck = duckdb.connect()\n",
    "_duck.execute(f\"CREATE OR REPLACE TABLE posts AS SELECT * FROM '{DATA_PATH}'\")\n",
    "_duck.register(\"dim_category\", _dim_pd)\n",
    "\n",
    "\n",
    "def query_a_pd():\n",
    "    return df_pd.groupby(\"category\").agg({\"likes\": \"mean\", \"views\": \"mean\"})\n",
    "\n",
    "def query_a_pl():\n",
    "    return df_pl.group_by(\"category\").agg([pl.col(\"likes\").mean(), pl.col(\"views\").mean()])\n",
    "\n",
    "def query_a_duck():\n",
    "    return _duck.execute(\"\"\"\n",
    "        SELECT category, avg(likes) AS avg_likes, avg(views) AS avg_views\n",
    "        FROM posts\n",
    "        GROUP BY category\n",
    "    \"\"\").fetch_df()\n",
    "\n",
    "def query_a_spark():\n",
    "    return df_spark.groupBy(\"category\").agg(F.avg(\"likes\"), F.avg(\"views\")).collect()\n",
    "\n",
    "\n",
    "def query_b_pd():\n",
    "    return (\n",
    "        df_pd.sort_values(\"timestamp\")\n",
    "        .assign(\n",
    "            likes_rank=df_pd.groupby(\"category\")[\"likes\"].rank(method=\"first\"),\n",
    "            views_dense_rank=df_pd.groupby(\"category\")[\"views\"].rank(method=\"dense\"),\n",
    "        )\n",
    "        .head(100)\n",
    "    )\n",
    "\n",
    "def query_b_pl():\n",
    "    return (\n",
    "        df_pl.sort(\"timestamp\")\n",
    "        .with_columns([\n",
    "            pl.col(\"likes\").rank(\"ordinal\").over(\"category\").alias(\"likes_rank\"),\n",
    "            pl.col(\"views\").rank(\"dense\").over(\"category\").alias(\"views_dense_rank\"),\n",
    "        ])\n",
    "        .head(100)\n",
    "    )\n",
    "\n",
    "def query_b_duck():\n",
    "    return _duck.execute(\"\"\"\n",
    "        SELECT category, timestamp, likes, views,\n",
    "               rank() OVER (PARTITION BY category ORDER BY likes) AS likes_rank,\n",
    "               dense_rank() OVER (PARTITION BY category ORDER BY views) AS views_dense_rank\n",
    "        FROM posts\n",
    "        ORDER BY timestamp\n",
    "        LIMIT 100\n",
    "    \"\"\").fetch_df()\n",
    "\n",
    "def query_b_spark():\n",
    "    return (\n",
    "        df_spark\n",
    "        .withColumn(\"likes_rank\", F.rank().over(Window.partitionBy(\"category\").orderBy(\"likes\")))\n",
    "        .withColumn(\"views_dense_rank\", F.dense_rank().over(Window.partitionBy(\"category\").orderBy(\"views\")))\n",
    "        .limit(100)\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "\n",
    "def query_c_pd():\n",
    "    return (\n",
    "        df_pd.merge(_dim_pd, on=\"category\")\n",
    "        .query(\"views > 10000 and likes > 100\")\n",
    "        .groupby(\"category\")\n",
    "        .agg({\"weight\": \"mean\", \"likes\": \"sum\"})\n",
    "    )\n",
    "\n",
    "\n",
    "def query_c_pl():\n",
    "    return (\n",
    "        df_pl.join(_dim_pl, on=\"category\")\n",
    "        .filter((pl.col(\"views\") > 10_000) & (pl.col(\"likes\") > 100))\n",
    "        .group_by(\"category\")\n",
    "        .agg([pl.col(\"weight\").mean(), pl.col(\"likes\").sum()])\n",
    "    )\n",
    "\n",
    "\n",
    "def query_c_duck():\n",
    "    return _duck.execute(\"\"\"\n",
    "        SELECT p.category, avg(d.weight) AS avg_weight, sum(p.likes) AS total_likes\n",
    "        FROM posts p\n",
    "        JOIN dim_category d USING (category)\n",
    "        WHERE p.views > 10000 AND p.likes > 100\n",
    "        GROUP BY p.category\n",
    "    \"\"\").fetch_df()\n",
    "\n",
    "\n",
    "def query_c_spark():\n",
    "    return (\n",
    "        df_spark.join(_dim_spark, on=\"category\")\n",
    "        .filter((F.col(\"views\") > 10000) & (F.col(\"likes\") > 100))\n",
    "        .groupBy(\"category\")\n",
    "        .agg(F.avg(\"weight\"), F.sum(\"likes\"))\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "QUERIES = {\n",
    "    \"QueryA\": {\"pandas\": query_a_pd, \"polars\": query_a_pl, \"duckdb\": query_a_duck, \"spark\": query_a_spark},\n",
    "    \"QueryB\": {\"pandas\": query_b_pd, \"polars\": query_b_pl, \"duckdb\": query_b_duck, \"spark\": query_b_spark},\n",
    "    \"QueryC\": {\"pandas\": query_c_pd, \"polars\": query_c_pl, \"duckdb\": query_c_duck, \"spark\": query_c_spark},\n",
    "}\n",
    "\n",
    "# ------------------ Benchmark helpers ------------------\n",
    "\n",
    "def _bench(run_fn):\n",
    "    t0 = time.time()\n",
    "    mem_trace, _ = memory_usage((run_fn, (), {}), retval=True, include_children=True, max_iterations=1)\n",
    "    elapsed = time.time() - t0\n",
    "    return elapsed, max(mem_trace)\n",
    "\n",
    "\n",
    "def _set_duck_threads(n):\n",
    "    _duck.execute(f\"PRAGMA threads={n}\")\n",
    "\n",
    "\n",
    "def _rebuild_spark(n):\n",
    "    global spark, df_spark\n",
    "    spark.stop()\n",
    "    spark = SparkSession.builder.appName(\"BigDataLab2\").master(f\"local[{n}]\").config(\"spark.driver.memory\", \"4g\").getOrCreate()\n",
    "    df_spark = spark.read.parquet(DATA_PATH)\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for engine in [\"pandas\", \"polars\", \"duckdb\", \"spark\"]:\n",
    "    thread_options = THREADS_TO_TEST if engine in (\"duckdb\", \"spark\") else [None]\n",
    "    for threads in thread_options:\n",
    "        if engine == \"duckdb\":\n",
    "            _set_duck_threads(threads)\n",
    "        if engine == \"spark\":\n",
    "            _rebuild_spark(threads)\n",
    "        for qname, registry in QUERIES.items():\n",
    "            fn = registry[engine]\n",
    "            elapsed, peak_mem = _bench(fn)\n",
    "            results.append({\n",
    "                \"engine\": engine,\n",
    "                \"threads\": threads or 1,\n",
    "                \"query\": qname,\n",
    "                \"time_s\": round(elapsed, 3),\n",
    "                \"peak_mb\": round(peak_mem, 2),\n",
    "            })\n",
    "            print(f\"{engine} | {qname} | threads={threads or 1} | time={elapsed:.3f}s | peak_mb={peak_mem:.2f}\")\n",
    "\n",
    "benchmark_df = pd.DataFrame(results)\n",
    "benchmark_df\n",
    "\n",
    "# ------------------ Speedup plot ------------------\n",
    "plot_df = benchmark_df[benchmark_df[\"threads\"] >= 1]\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "for (engine, query), grp in plot_df.groupby([\"engine\", \"query\"]):\n",
    "    base = grp.loc[grp[\"threads\"] == grp[\"threads\"].min(), \"time_s\"].iloc[0]\n",
    "    ax.plot(grp[\"threads\"], base / grp[\"time_s\"], marker=\"o\", label=f\"{engine}-{query}\")\n",
    "ax.set_xlabel(\"Threads\")\n",
    "ax.set_ylabel(\"Speedup vs. 1 thread\")\n",
    "ax.set_xscale(\"log\", base=2)\n",
    "ax.set_title(\"Single-node speedup per query/engine\")\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f48ffba51c7c3a",
   "metadata": {},
   "source": [
    "### Task 2: Spark on Cluster\n",
    "\n",
    "**Goal**: Compare Single Node performance vs. Spark on a Cluster.\n",
    "\n",
    "**Instructions**:\n",
    "1.  **Infrastructure**: Use the infrastructure from **Phase 1** (Google Dataproc). You may need to modify your Terraform code to adjust the cluster configuration (e.g., number of worker nodes).\n",
    "2.  **Environment**: The easiest way to run this is via **Google Workbench** connected to your Dataproc cluster.\n",
    "3.  **Upload Data**: Upload the generated `social_media_data.parquet` to HDFS or GCS.\n",
    "    -   **Tip**: For better performance, consider **partitioning** the data (e.g., by `category` or `date`) when saving it to the distributed storage. This allows Spark to optimize reads.\n",
    "4.  **Run Queries**: Run your PySpark queries from Task 1 on the cluster.\n",
    "5.  **Scalability Test**: \n",
    "    -   Run the queries with different numbers of **worker nodes** (e.g., 2, 3, 4).\n",
    "    -   You can achieve this by resizing the cluster (manually or via Terraform) or by configuring the number of executors in Spark.\n",
    "6.  **Analyze**:\n",
    "    -   How does the cluster performance compare to your local machine?\n",
    "    -   Did adding more nodes/executors linearly improve performance?\n",
    "    -   **Tip**: If Spark is slower than single-node engines, consider **increasing the dataset size** (e.g., generate 10M+ records or duplicate the data). Spark's overhead is significant for small data, and its true power appears when data exceeds single-node memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40672b4a6e5fa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"gs://tbd-2025z-319020-dataproc-staging/data/social_media_data.parquet\"# Your Code Here for Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb0f29d88d7c0f2",
   "metadata": {},
   "source": [
    "### Task 3: Execution Modes & Analysis\n",
    "\n",
    "**Goal**: Deep dive into execution models and limitations.\n",
    "\n",
    "**Instructions**:\n",
    "1.  **Lazy vs. Eager vs. Streaming**:\n",
    "    -   Use **Polars**. Compare the **Execution Time** and **Peak Memory** of:\n",
    "        -   Eager execution (`read_parquet` -> filter).\n",
    "        -   Lazy execution (`scan_parquet` -> filter -> `collect()`).\n",
    "        -   Streaming execution (`scan_parquet` -> filter -> `collect(streaming=True)`).\n",
    "2.  **Polars Limitations**:\n",
    "    -   Identify a scenario where Polars might struggle compared to Spark (e.g., memory limits).\n",
    "3.  **Decision Boundary**:\n",
    "    -   Based on your findings, when would you recommend switching from a single-node tool (Polars/DuckDB) to a distributed engine (Spark)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11652ae7fcf51be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here for Task 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
